%
% File eacl2017.tex
%
%% Based on the style files for ACL-2016
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{eacl2017}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
%\usepackage{polski}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{booktabs}

%\usepackage{todonotes}%remove this when all todos are gone
\usepackage{todonotes}

\newcommand{\comment}[1]{}
\newcommand{\COMMENT}[1]{\comment{#1}}

\newcommand{\nocomment}[1]{{#1}}
\newcommand{\SHORTEN}[1]{}
\newcommand{\textexample}[1]{{\em #1}}
\newcommand{\dontsubmit}[1]{{\color{blue} #1}} %Change this to {} once ready for submission
%\newcommand{\dontsubmit}[1]{} %Change this to {} once ready for submission
\newcommand{\URGENT}[1]{\dontsubmit{\bf \color{red} #1}} %{{\bf \color{red} #1}}% % replace by {} when done




\eaclfinalcopy % Uncomment this line for the final submission
%\def\eaclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Shared Task on Multilingual Named Entity Recognition, Normalisation and Cross-language Matching for Slavic Languages}


\author{Jakub Piskorski \\
  Joint Research Centre \\
	Via Enrico Fermi 2749 \\
  21027 Ispra (VA), Italy \\
  {\tt firstname.lastname} \\
	{\tt @jrc.ec.europa.eu} \\\And
  Lidia Pivovarova \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And 
	Jan Å najder \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And	
	Josef Steinberger \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
	Roman Yangarber \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} 
	\\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
This paper describes the outcomes of the shared task on multilingual named entity recognition 
that aimed at recognizing mentions of named entities in web documents in Slavic languages, 
their normalization/lemmatization, and cross-language matching. The task was organised in 
the context of the 6-th Balto-Slavic Natural Language Processing Workshop co-located with 
EACL 2017 conference. 
\todo{amend once we know the results}
\end{abstract}

\section{Introduction}
\label{sec:intro}

Due to rich inflection, derivation, free word order, and other phenomena
exhibited by Slavic languages, the detection of names and their
lemmatization poses a challenging task. Fostering research and
development on this problem---and the closely related problem of entity
linking---is of paramount importance for enabling effective multilingual
and cross-lingual information access in these languages.

\URGENT{TODO:}

\begin{itemize}

\item motivation

\item aim and the task

\item related shared tasks

\end{itemize}

Several related shared tasks were organised in the past. While the first non-English monolingual 
NER evaluation campaigns (covering Chinese, Japanese, Spanish and Arabic) were carried out in the 
context of MUC conferences~\cite{chinchor:98} and ACE Programme~\cite{conf/lrec/DoddingtonMPRSW04},
the first shared task focusing on multilingual named entity recognition, which covered evaluation of
some European languages, including e.g., Spanish, German Dutch, etc. were organised in the context of
CoNLL 2002~\cite{TjongKimSang:2002:ICS:1118853.1118877} and 2003 conferences~\cite{TjongKimSang:2002:ICS:1118853.1118877}.
The NE types covered in the aforementioned campaigns were similar to NE types covered in our
shared task. Somewhat related to our task is the Entity Discovery and Linking (EDL) track 
which has been part of the The Text Analysis Conferences (TAC) that aims to extract entity mentions 
from a source collection of textual documents in multiple languages (English, Chinese, and Spanish), 
and creating cross-document entity equivalence classes by either linking each mention to a knowledge base or
directly clustering them.~\cite{ji:ea:2014,ji:ea:2015} The fundamental difference between EDL and our task
is that the latter does not include the sub-task of linking entities to a knowledge base. 
To our best knowledge the shared task described in this paper is a first attempt on multilingual
name recognition, their normalization and cross-language entity matching that covers Slavic
languages in particular.

The rest of this paper is organised as follows. In Section~\ref{sec:task}, the task is
introduced.  Next, in Section~\ref{sec:protocol} the system response
protocol is described.  Subsequently, the characteristics of the test
datasets are provided in Section~\ref{sec:annotation}.  The evaluation
methodology is introduced in Section~\ref{sec:evaluation}.  Next, the
description of participant systems is provided in
Section~\ref{sec:participants} and the results obtained by these systems
are presented in Section~\ref{sec:results}.  Finally, lessons learnt and
main conclusions are discussed in Section~\ref{sec:conclusions}.

\section{Task Description}
\label{sec:task}


We are given a collection of text documents in several Slavic languages
extracted from the Web and focusing around a certain entity---e.g., a
person or an organization.  The task is to recognize, classify, and
``normalize'' all named-entity mentions in each of the documents, and to
link together across languages all named mentions referring to the same
real-world entity.  By ``normalization'' we mean lemmatization, return to
a basic, dictionary form.  The document collection is obtained by posing
a query to a search engine, and parsing the HTML of returned documents.
Formally, the Multilingual Named Entity Recognition task is
subdivided into three sub-tasks:

\begin{itemize}

\item \textbf{Named Entity Mention Detection and Classification}:
  recognizing all unique named mentions of entities of four types:
  persons (PER), organizations (ORG), locations (LOC), miscellaneous
  (MISC), the last covering mentions of all other types of named
  entities, e.g., products, events, etc.

\item \textbf{Name Normalization}: computing for each detected named mention of an entity its corresponding base form/lemma, and

\item \textbf{Entity Matching}: assigning to each detected named mention of an entity an identifier in such a way that detected 
  mentions of entities referring to the same real-world entity should be assigned the same identifier, which is referred to as 
  cross-lingual ID.

\end{itemize}

\noindent The task does not include returning positional information of name entity mentions. Consequently,
for all occurrences of the same form of a mention (e.g., inflected variant, acronym or abbreviation) of a named entity 
within the same document not more than one annotation should be returned, unless the different occurrences thereof have 
different entity types (different readings) assigned to them. Furthermore, distinguishing case information is not relevant
since the evaluation is case-insensitive. In particular, if the text includes lowercase, uppercase and/or mixed-letter 
named mention variants of the same entity, the system response should include only one annotation for all of these mentions. 
For instance, for ``\textit{ISIS}'', ``\textit{isis}'', and ``\textit{Isis}'' (provided that they refer to the same named-entity type), 
only one annotation should be returned (e.g., ``\textit{Isis}'').

It is important to emphasize that the recognition of nominal or pronominal mentions of entities is not part of the task. 

As regards language coverage the shared task covered recognition of names in seven Slavic languages, namely, Croatian, 
Czech, Polish, Russian, Slovak, Slovene, Ukrainian. It is envisioned to extend the task to cover additional Slavic 
languages at a later stage, which goes beyond the scope of the task defined for the 2017 edition of BSNLP workshop.

As regards recognition of particular types of named-entity the following rules were imposed:

\begin{itemize}

\item \textbf{Person names (PER)}:

  a) Person names should not include titles, honorifics, and functions/positions. For example, in the text fragment ``\textit{CEO Dr. Jan Kowalski}'', only ``\textit{Jan Kowalski"}''' should be recognized as a person name. However, initials and pseudonyms are considered named mentions of person names and should be recognized. Similarly, named references to groups of people (that do not have a formal organization unifying them) should also be recognized, e.g., ``\textit{Ukrainians}.''

  b) Personal possessives derived from a named mention of a person should
  be classified as a person, and the base form of the corresponding
  person name should be extracted. For instance, for
  ``\textit{Piskorskijev mejl}'' (Croatian) it is expected to recognize
  ``\textit{Piskorskijev}'', classified with PER and extract the base
  form: ``\textit{Piskorski}.''

  c) Fictive persons and characters are considered as persons.

\item \textbf{Locations (LOC)}:

  \begin{itemize}

  \item This category includes all toponyms and geopolitical entities
    (e.g., cities, counties, provinces, countries, regions, bodies of
    water, land formations, etc.) and named mentions of facilities (e.g.,
    stadiums, parks, museums, theaters, hotels, hospitals, transportation
    hubs, churches, railroads, bridges, and other similar urban and
    non-urban facilities).

  \item Even in case of named mentions of facilities that refer to an
    organization, the LOC tag should be used. For example, from the text
    phrase ``\textit{The Schipol airport has acquired new electronic
      gates}'' the mention ``\textit{The Schipol airport}'' should be
    extracted and classified as LOC.

  \end{itemize}

\item \textbf{Organizations (ORG)}:

  \begin{itemize}

  \item This category covers all kind of organizations such as: political
    parties, public institutions, international organizations, companies,
    religious organizations, sport organizations, education and research
    institutions, etc.

  \item Organization designators and potential mentions of the seat of
    the organization are considered to be part of the organization
    name. For instance, from the text fragment ``\textit{Citi Handlowy w
      Poznaniu}'' (a bank in PoznaÅ), the full phrase ``\textit{Citi
      Handlowy w Poznaniu}'' should be extracted.

  \end{itemize}

\item \textbf{Miscellaneous (MISC)}:

  \begin{itemize}

  \item This category covers all other named mentions of entities, e.g.,
    product names (e.g., ``\textit{Motorola Moto X}''), events
    (conferences, concerts, natural disasters, holidays, e.g.,
    ``\textit{ÅwiÄta BoÅ¼ego Narodzenia}'', etc.).

  \item This category does not include the recognition of temporal and
    numerical expressions, as well as recognition of identifiers such as
    email addresses, URLs, postal addresses, etc.

  \end{itemize}

\end{itemize}

\noindent \textbf{Other aspects}:

\begin{itemize}

\item In case of complex named entities, consisting of nested named
  entities, only the top-most entity should be recognized. For example,
  from the text fragment ``\textit{George Washington University}'' one
  should not extract ``\textit{George Washington}'', but the entire name,
  namely, ``\textit{George Washington University}.''

\item In case a same form (e.g., ``\textit{Washington}'') in two different
  contexts in the same document is used to refer to two different
  real-world entities (e.g. a person and a location) the system should
  return two annotations respectively, where each of them associates the
  mention with a different cross-language ID resp.

\end{itemize}


The registered participants were provided two trial datasets related to:
(a) {\em Beata SzydÅo}, the current prime minister of Poland, and (b)
ISIS, the so-called ``Islamic State of Iraq and Syria'' terror group.
These datasets consisted of 187 and 186 documents respectively, with
equal distribution of documents across the seven languages of interest.

\section{System Input and Response Protocol}
\label{sec:protocol}

\subsection{Input Document Format}
\label{subsec:input}

Documents in trial and test datasets are represented in the following
format, where the first 5 lines contain metadata, whereas the core text
to be processed is available from the 6th line till the end of the file.

\begin{small}
\begin{verbatim}
<DOCUMENT-ID>
<LANGUAGE>
<CREATION-DATE>
<URL>
<TITLE>
<TEXT>
\end{verbatim}
\end{small}

\noindent The \verb+<URL>+ field was used to store information on the origin from which 
the text document was created. The values of the metadata fields were computed automatically 
(see~\ref{sec:annotation} for details). In particular, the values of the \verb+<CREATION-DATE>+ 
and \verb+<TITLE>+ fields were not provided for all documents either due to unavailability 
of such data or due to web page parsing errors during the creation process. 

\subsection{System Response}
\label{subsec:input}

For each input document the systems should return a corresponding file in the following format.
The first line should contain only the \verb+<DOCUMENT-ID>+ field that corresponds to the input file,
whereas each subsequent line should be of the format:

\begin{small}
\begin{verbatim}
<MENTION> TAB <BASE> TAB <CAT> TAB <ID>
\end{verbatim}
\end{small}

\noindent The value of the \verb+<MENTION>+ field should contain the named-entity mention as it 
appears in text. The value of the \verb+<BASE>+ field should contain the base form of the
entity. Finally, the \verb+<CAT>+ and \verb+<ID>+ fields are used to store information
on the category of the entity (ORG, PER, LOC or MISC) and cross-lingual identifier resp. 
The cross-lingual identifiers may consist of an arbitrary sequence of alphanumeric characters.
An example of system response (for a document in Polish) is given below.

\begin{small}
\begin{verbatim}
16
Podlascy Czeczeni Podlascy Czeczeni PER 1
ISIS ISIS ORG 2
Rosji Rosja LOC 3
Polsce Polska LOC 4
Warszawie Warszawa LOC 5
Rosja Rosja LOC 3
Magazynu Kuriera Porannego \
   Magazyn Kuriera Porannego ORG 6
\end{verbatim}	    
\end{small}

\section{Task Dataset}
\label{sec:annotation}

Two datasets were prepared for evaluation, each consisting of documents
extracted from the Web and revolving around a given entity.
Specifically, the first dataset contains documents related to Donald
Trump, the recently elected President of United States (which will be
referred to as {\sc Trump}), whereas the second dataset contains
documents related to the European Commission (which will be referred to
as {\sc ECommission}).

The test datasets were created in the following manner.  For each entity
of interest we have posed a separate search query to Google, in each of
the seven target languages.  The query was configured to return links to
documents only in the language of interest.  Next, we extracted the first
100 links\footnote{Or fewer, in case the search engine did not return 100
  links.} returned by the search engine, removed duplicate links and
downloaded the corresponding HTML pages---mainly news articles or
fragments thereof---and converted them into pure text, using a hybrid
HTML parser.  This process was done semi-automatically using the tool
described in~\cite{Crawley:ea:2010}.  In particular, some of the metadata
fields---i.e., creation date, title, URL---were automatically computed
using this tool.


The HTML parsing resulted in extracting texts that may include not only
the core text of a Web page, but also some additional pieces of text,
e.g., a list of labels from a menu, user comments, etc., which might not
necessarily constitute well-formed utterances in the target language.
This occurred in a small fraction of texts processed.  Some of
these texts were included in the test dataset in order to maintain the
flavour of ``real-data.''  However, obvious HTML parser failure, e.g.,
extraction of JavaScript code, extraction of empty texts, etc., were removed
from the data sets.  Some of the downloaded documents were
additionally polished by removing erroneously extracted boilerplate
content.  The resulting set of partially ``cleaned'' documents were used to
select for each language and topic circa 20-25 documents for the
preparation of the final test datasets. The annotation for Croatian,
Czech, Polish, Russian and Slovene were done by native speakers, whereas
annotation for Slovak and Ukrainian were made by native speakers of Czech
and Russian, respectively, capable of understanding the former
languages.  The cross-language alignment of the entity identifiers was
done by two annotators.  Table~\ref{tab:datasets} provides more detailed
quantitative data about the test annotated datasets.

\begin{table}
\begin{center}
\begin{footnotesize}
%\begin{tabular}{|l|c|c|c|c|c|c|}
\begin{tabular}{lcccc}
\toprule 
 & \multicolumn{2}{c}{\textbf{{\sc Trump}}} & \multicolumn{2}{c}{\textbf{{\sc ECommission}}} \\
\cmidrule(lr){2-3}
\cmidrule(lr){4-5}
Language &  \#\,docs & \#\,ment & \#\,docs & \#\,ment \\
\midrule
Croatian & 522 & 25 & x & x \\
Czech & 479 & 25 & x & x \\
Polish & 692 & 25 & x & x \\
Russian & 331 & 26 & x & x \\
Slovak  & 453 & 24 & x & x \\
Slovene & 474 & 24 & x & x \\
Ukrainian & 337 & 28 & x & x \\
\midrule
Total & 3288 & 177 & x & x  \\


\bottomrule
\end{tabular}
\end{footnotesize}
\end{center}
\caption{Quantitative data about the test datasets. \#docs and \#ment refer to the number of documents and named-entity mention annotations.}
\label{tab:datasets}
\end{table}

\todo{provide some figures on the fraction of ORGS, PER, LOCs
  etc. in the datasets}

\section{Evaluation Methodology}
\label{sec:evaluation}

Named entity recognition (exact case-insensitive matching) and  Name Normalization (sometimes called âlemmatizationâ) tasks were evaluated in terms of precision, recall, and F1 scores. In particular, as regards named entity recognition, two types of evaluations were carried out:

\begin{itemize}

\item \textbf{Relaxed evaluation}: an entity mentioned in a given document is considered to be extracted correctly if the system response includes at least one annotation of a named mention of this entity (regardless whether the extracted mention is base form);

\item \textbf{Strict evaluation}: the system response should include exactly one annotation for each unique form of a named mention of an entity that is referred to in a given document, i.e., capturing and listing all variants of an entity is required.

\end{itemize}

In the context of the evaluation we considered various level of granularity, i.e., the performance for: 
(a) all NE types and all languages, (b) each particular NE type and all languages, (c) all NE types for each language,
and (d) each particular NE type per language. 

As regards the evaluation of document-level and cross-language entity matching task we have adapted the 
Link-Based Entity-Aware metric (LEA)~\cite{DBLP:conf-acl-Moosavi016}.

The evaluation was carried out in ``case-insensitive'' mode, all named mentions in system response and test corpora were 
lowercased.

\todo{extend, in particular on LEA}

\section{Participant Systems}
\label{sec:participants}

\begin{itemize}

\item how many teams registered, submitted results, which sub tasks they covered, etc.

\item short summary on what technique(s) each of the participating system is based

\end{itemize}

11 teams signed up for the evaluation task and received the trial datasets and 
only 3 (\textbf{AS OF NOW} teams) submitted results. A total of \textbf{XX} 
unique runs were submitted: ...

The {\sc DeepNLP} team~\footnote{https://github.com/tindzk/bsnlp} learns entities and their base forms from 
Wikipedia data dumps.~\footnote{https://dumps.wikimedia.org/} A character-level recurrent neural network~\cite{DBLP:conf/aaai/KimJSR16} 
is trained to predict which characters belong to an entity. A second neural network is trained on inflected entities and predicts their base forms.

JHU/APL team only attempted the NER and Entity Matching sub-tasks. They employed a statistical tagger called 
SVMLattice~\cite{Mayfield:2003:LTU:956863.956921}, with NER labels inferred by projecting English tags across bitext. 
The Illinois tagger~\cite{Ratinov:2009:DCM:1596374.1596399} was used for English. A rule-based entity clusterer 
called "kripke" was used for Entity Matching~\cite{DBLP:conf/tac/McNameeMFL13}.

{\sc G4.19 Research Group} adapted Liner2~\cite{series/sci/MarcinczukKJ13} - a generic framework 
which can be used to solve various tasks based on sequence labeling, which is equipped
with a set of modules (based on statistical models, dictionaries, rules and heuristics) 
which recognize and annotate certain types of phrases. The details of tuning Liner2 to
tackle the shared task is described in~\cite{marcinczuk:2017}.

\section{Evaluation Results}
\label{sec:results}

\todo{content and presentation yet to be elaborated}

\section{Conclusions}
\label{sec:conclusions}

\todo{TODO:}

\section*{Acknowledgments}


We would like to thank Katja Zupan from the Department of Knowledge Technologies from
JoÅ¾ef Stefan Institute in Slovenia, Anastasia Stepanova from the State University of New York at Buffalo, 
and Olga Kanishcheva, Kateryna Klymenkova, Ekaterina Yurieva from the National Technical University 
(Kharkiv Polytechnic Institute), who contributed to preparation of the Slovenian, Russian and Ukrainian test 
datasets. We are also indebted to TomaÅ¾ Erjavec from the Department of Knowledge Technologies from JoÅ¾ef Stefan 
Institute in Slovenia, who contributed with various ideas.




\bibliographystyle{eacl2017}
\bibliography{sharedBSNLP2017}




\end{document}
