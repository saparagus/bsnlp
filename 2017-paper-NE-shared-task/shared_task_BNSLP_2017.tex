%
% File eacl2017.tex
%
%% Based on the style files for ACL-2016
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{eacl2017}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{amsmath}
%\usepackage{polski}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{booktabs}

%\usepackage{todonotes}%remove this when all todos are gone
\usepackage{todonotes}

\newcommand{\comment}[1]{}
\newcommand{\COMMENT}[1]{\comment{#1}}

\newcommand{\nocomment}[1]{{#1}}
\newcommand{\SHORTEN}[1]{}
\newcommand{\textexample}[1]{{\em #1}}
\newcommand{\dontsubmit}[1]{{\color{blue} #1}} %Change this to {} once ready for submission
%\newcommand{\dontsubmit}[1]{} %Change this to {} once ready for submission
\newcommand{\URGENT}[1]{\dontsubmit{\bf \color{red} #1}} %{{\bf \color{red} #1}}% % replace by {} when done




\eaclfinalcopy % Uncomment this line for the final submission
%\def\eaclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Shared Task on Multilingual Named Entity Recognition, Normalisation and Cross-language Matching for Slavic Languages}


\author{Jakub Piskorski \\
  Joint Research Centre \\
	Via Enrico Fermi 2749 \\
  21027 Ispra (VA), Italy \\
  {\tt firstname.lastname} \\
	{\tt @jrc.ec.europa.eu} \\\And
  Lidia Pivovarova \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And 
	Jan Å najder \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And	
	Josef Steinberger \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
	Roman Yangarber \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} 
	\\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
This paper describes the outcomes of the shared task on multilingual named entity recognition 
that aimed at recognizing mentions of named entities in web documents in Slavic languages, 
their normalization/lemmatization, and cross-language matching. The task was organised in 
the context of the 6-th Balto-Slavic Natural Language Processing Workshop co-located with 
EACL 2017 conference. 
\todo{amend once we know the results}
\end{abstract}

\section{Introduction}
\label{sec:intro}

Due to rich inflection, derivation, free word order, and other phenomena
exhibited by Slavic languages, the detection of names and their
lemmatization poses a challenging task.~\cite{Przepiorkowski:2007:SIE:1567545.1567547,journals/ir/PiskorskiWS09} 
Fostering research and development on this problem---and the closely related problem of entity
linking---is of paramount importance for enabling effective multilingual
and cross-lingual information access in these languages.

This paper describes the outcomes of the shared task on multilingual named 
entity recognition that aimed at recognizing mentions of named entities in web documents 
in Slavic languages, their normalization/lemmatization, and cross-language matching.
The task initially covers seven languages and focuses on recognition of four types of 
named entities including: persons, locations, organizations, and miscellaneous,
where the last category covers mentions of all other types of named entities, e.g., events and products.
The input text collection consists of sets of documents in various languages from the 
Web, each collection revolving around a certain entity. The main rationale of such a 
set-up is to foster development of ``allrounder'' named-entity recognition and 
cross-language entity matching solutions that are not tailored to specific domains.
The shared task was organised in the context of the 6-th Balto-Slavic Natural Language Processing 
Workshop co-located with EACL 2017 conference. 

Several related shared tasks were organised in the past. While the first non-English monolingual 
NER evaluation campaigns (covering Chinese, Japanese, Spanish and Arabic) were carried out in the 
context of MUC conferences~\cite{chinchor:98} and ACE Programme~\cite{conf/lrec/DoddingtonMPRSW04},
the first shared task focusing on multilingual named entity recognition, which covered evaluation of
some European languages, including e.g., Spanish, German Dutch, etc. were organised in the context of
CoNLL 2002~\cite{TjongKimSang:2002:ICS:1118853.1118877} and 2003 conferences~\cite{TjongKimSang:2003:ICS:1119176.1119195}.
The NE types covered in the aforementioned campaigns were similar to NE types covered in our
shared task. Somewhat related to our task is the Entity Discovery and Linking (EDL) track~\cite{ji:ea:2014,ji:ea:2015} 
which has been part of the NIST Text Analysis Conferences (TAC) that aims to extract entity mentions 
from a source collection of textual documents in multiple languages (English, Chinese, and Spanish), 
and to create cross-document entity equivalence classes by either linking each mention to a knowledge base or
directly clustering them. The fundamental difference between EDL and our task
is that the latter does not include the sub-task of linking entities to a knowledge base. 
To our best knowledge the shared task described in this paper is a first attempt on multilingual
name recognition, their normalization and cross-language entity matching that covers Slavic
languages in particular.

The rest of this paper is organised as follows. In Section~\ref{sec:task}, the task is
introduced.  Next, in Section~\ref{sec:protocol} the system response
protocol is described.  Subsequently, the characteristics of the test
datasets are provided in Section~\ref{sec:annotation}.  The evaluation
methodology is introduced in Section~\ref{sec:evaluation}.  Next, the
description of participant systems is provided in
Section~\ref{sec:participants} and the results obtained by these systems
are presented in Section~\ref{sec:results}.  Finally, lessons learnt and
main conclusions are discussed in Section~\ref{sec:conclusions}.

\section{Task Description}
\label{sec:task}


We are given a collection of text documents in several Slavic languages
extracted from the Web and focusing around a certain entity---e.g., a
person or an organization.  The task is to recognize, classify, and
``normalize'' all named-entity mentions in each of the documents, and to
link together across languages all named mentions referring to the same
real-world entity.  By ``normalization'' we mean lemmatization, return to
a basic, dictionary form.  The document collection is obtained by posing
a query to a search engine, and parsing the HTML of returned documents.
Formally, the Multilingual Named Entity Recognition task is
subdivided into three sub-tasks:

\begin{itemize}

\item \textbf{Named Entity Mention Detection and Classification}:
  recognizing all unique named mentions of entities of four types:
  persons (PER), organizations (ORG), locations (LOC), miscellaneous
  (MISC), the last covering mentions of all other types of named
  entities, e.g., products, events, etc.

\item \textbf{Name Normalization}: computing for each detected named mention of an entity its corresponding base form/lemma, and

\item \textbf{Entity Matching}: assigning to each detected named mention of an entity an identifier in such a way that detected 
  mentions of entities referring to the same real-world entity should be assigned the same identifier, which is referred to as 
  cross-lingual ID.

\end{itemize}

\noindent The task does not include returning positional information of name entity mentions. Consequently,
for all occurrences of the same form of a mention (e.g., inflected variant, acronym or abbreviation) of a named entity 
within the same document not more than one annotation should be returned, unless the different occurrences thereof have 
different entity types (different readings) assigned to them. Furthermore, distinguishing case information is not relevant
since the evaluation is case-insensitive. In particular, if the text includes lowercase, uppercase and/or mixed-letter 
named mention variants of the same entity, the system response should include only one annotation for all of these mentions. 
For instance, for ``\textit{ISIS}'', ``\textit{isis}'', and ``\textit{Isis}'' (provided that they refer to the same named-entity type), 
only one annotation should be returned (e.g., ``\textit{Isis}'').

It is important to emphasize that the recognition of nominal or pronominal mentions of entities is not part of the task. 

As regards language coverage the shared task covered recognition of names in seven Slavic languages, namely, Croatian, 
Czech, Polish, Russian, Slovak, Slovene, Ukrainian. It is envisioned to extend the task to cover additional Slavic 
languages at a later stage, which goes beyond the scope of the task defined for the 2017 edition of BSNLP workshop.

As regards recognition of particular types of named-entity the following rules were imposed:

\begin{itemize}

\item \textbf{Person names (PER)}:

  a) Person names should not include titles, honorifics, and functions/positions. For example, in the text fragment ``\textit{CEO Dr. Jan Kowalski}'', only ``\textit{Jan Kowalski"}'' should be recognized as a person name. However, initials and pseudonyms are considered named mentions of person names and should be recognized. Similarly, named references to groups of people (that do not have a formal organization unifying them) should also be recognized, e.g., ``\textit{Ukrainians}''. In this context, mentions of a single member belonging to such groups, e.g. ``\textit{Ukrainian}'', should be assigned the same cross-language ID as plural mentions, i.e., ``\textit{Ukrainians}'' and
``\textit{Ukrainian}'' when referring to the nation should be assigned the same cross-language ID.	

  b) Personal possessives derived from a named mention of a person should
  be classified as a person, and the base form of the corresponding
  person name should be extracted. For instance, for
  ``\textit{Piskorskijev mejl}'' (Croatian) it is expected to recognize
  ``\textit{Piskorskijev}'', classified with PER and extract the base
  form: ``\textit{Piskorski}''.

  c) Fictive persons and characters are considered as persons.

\item \textbf{Locations (LOC)}:

  \begin{itemize}

  \item This category includes all toponyms and geopolitical entities
    (e.g., cities, counties, provinces, countries, regions, bodies of
    water, land formations, etc.) and named mentions of facilities (e.g.,
    stadiums, parks, museums, theaters, hotels, hospitals, transportation
    hubs, churches, railroads, bridges, and other similar urban and
    non-urban facilities).

  \item Even in case of named mentions of facilities that refer to an
    organization, the LOC tag should be used. For example, from the text
    phrase ``\textit{The Schipol airport has acquired new electronic
      gates}'' the mention ``\textit{The Schipol airport}'' should be
    extracted and classified as LOC.

  \end{itemize}

\item \textbf{Organizations (ORG)}:

  \begin{itemize}

  \item This category covers all kind of organizations such as: political
    parties, public institutions, international organizations, companies,
    religious organizations, sport organizations, education and research
    institutions, etc.

  \item Organization designators and potential mentions of the seat of
    the organization are considered to be part of the organization
    name. For instance, from the text fragment ``\textit{Citi Handlowy w
      Poznaniu}'' (a bank in PoznaÅ), the full phrase ``\textit{Citi
      Handlowy w Poznaniu}'' should be extracted.
			
  \item When a company name is used to refer to a service (e.g., ``\textit{na
Twiterze}'' (Polish) - on Twitter) the mention of ``\textit{Twitter}'' is considered to 
refer to a service/product and should be tagged as MISC. However, when a company name 
is referring to a service which expresses the opinion of the company, e.g. ``\textit{Fox News}'', 
it should be tagged as ORG.			

  \end{itemize}

\item \textbf{Miscellaneous (MISC)}:

  \begin{itemize}

  \item This category covers all other named mentions of entities, e.g.,
    product names (e.g., ``\textit{Motorola Moto X}''), events
    (conferences, concerts, natural disasters, holidays, e.g.,
    ``\textit{ÅwiÄta BoÅ¼ego Narodzenia}'', etc.).

  \item This category does not include the recognition of temporal and
    numerical expressions, as well as recognition of identifiers such as
    email addresses, URLs, postal addresses, etc.

  \end{itemize}

\end{itemize}

\noindent \textbf{Other aspects}:

\begin{itemize}

\item In case of complex named entities, consisting of nested named
  entities, only the top-most entity should be recognized. For example,
  from the text fragment ``\textit{George Washington University}'' one
  should not extract ``\textit{George Washington}'', but the entire name,
  namely, ``\textit{George Washington University}.''

\item In case a same form (e.g., ``\textit{Washington}'') in two different
  contexts in the same document is used to refer to two different
  real-world entities (e.g. a person and a location) the system should
  return two annotations respectively, where each of them associates the
  mention with a different cross-language ID resp.

\end{itemize}


The registered participants were provided two trial datasets related to:
(a) {\em Beata SzydÅo}, the current prime minister of Poland, and (b)
ISIS, the so-called ``Islamic State of Iraq and Syria'' terror group.
These datasets consisted of 187 and 186 documents respectively, with
equal distribution of documents across the seven languages of interest.

\section{System Input and Response Protocol}
\label{sec:protocol}

\subsection{Input Document Format}
\label{subsec:input}

Documents in trial and test datasets are represented in the following
format, where the first 5 lines contain metadata, whereas the core text
to be processed is available from the 6th line till the end of the file.

\begin{small}
\begin{verbatim}
<DOCUMENT-ID>
<LANGUAGE>
<CREATION-DATE>
<URL>
<TITLE>
<TEXT>
\end{verbatim}
\end{small}

\noindent The \verb+<URL>+ field was used to store information on the origin from which 
the text document was created. The values of the metadata fields were computed automatically 
(see~\ref{sec:annotation} for details). In particular, the values of the \verb+<CREATION-DATE>+ 
and \verb+<TITLE>+ fields were not provided for all documents either due to unavailability 
of such data or due to web page parsing errors during the creation process. 

\subsection{System Response}
\label{subsec:input}

For each input document the systems should return a corresponding file in the following format.
The first line should contain only the \verb+<DOCUMENT-ID>+ field that corresponds to the input file,
whereas each subsequent line should be of the format:

\begin{small}
\begin{verbatim}
<MENTION> TAB <BASE> TAB <CAT> TAB <ID>
\end{verbatim}
\end{small}

\noindent The value of the \verb+<MENTION>+ field should contain the named-entity mention as it 
appears in text. The value of the \verb+<BASE>+ field should contain the base form of the
entity. Finally, the \verb+<CAT>+ and \verb+<ID>+ fields are used to store information
on the category of the entity (ORG, PER, LOC or MISC) and cross-lingual identifier resp. 
The cross-lingual identifiers may consist of an arbitrary sequence of alphanumeric characters.
An example of system response (for a document in Polish) is given below.

\begin{small}
\begin{verbatim}
16
Podlascy Czeczeni Podlascy Czeczeni PER 1
ISIS ISIS ORG 2
Rosji Rosja LOC 3
Polsce Polska LOC 4
Warszawie Warszawa LOC 5
Rosja Rosja LOC 3
Magazynu Kuriera Porannego \
   Magazyn Kuriera Porannego ORG 6
\end{verbatim}	    
\end{small}

\section{Task Dataset}
\label{sec:annotation}

Two datasets were prepared for evaluation, each consisting of documents
extracted from the Web and revolving around a given entity.
Specifically, the first dataset contains documents related to Donald
Trump, the recently elected President of United States (which will be
referred to as {\sc Trump}), whereas the second dataset contains
documents related to the European Commission (which will be referred to
as {\sc ECommission}).

The test datasets were created in the following manner.  For each entity
of interest we have posed a separate search query to Google, in each of
the seven target languages.  The query was configured to return links to
documents only in the language of interest.  Next, we extracted the first
100 links\footnote{Or fewer, in case the search engine did not return 100
  links.} returned by the search engine, removed duplicate links and
downloaded the corresponding HTML pages---mainly news articles or
fragments thereof---and converted them into pure text, using a hybrid
HTML parser.  This process was done semi-automatically using the tool
described in~\cite{Crawley:ea:2010}.  In particular, some of the metadata
fields---i.e., creation date, title, URL---were automatically computed
using this tool.


The HTML parsing resulted in extracting texts that may include not only
the core text of a Web page, but also some additional pieces of text,
e.g., a list of labels from a menu, user comments, etc., which might not
necessarily constitute well-formed utterances in the target language.
This occurred in a small fraction of texts processed.  Some of
these texts were included in the test dataset in order to maintain the
flavour of ``real-data.''  However, obvious HTML parser failure, e.g.,
extraction of JavaScript code, extraction of empty texts, etc., were removed
from the data sets.  Some of the downloaded documents were
additionally polished by removing erroneously extracted boilerplate
content.  The resulting set of partially ``cleaned'' documents were used to
select for each language and topic circa 20-25 documents for the
preparation of the final test datasets. The annotation for Croatian,
Czech, Polish, Russian, Slovene and Ukrainian were done by native speakers, whereas
annotation for Slovak and Ukrainian were made by native speakers of Czech, capable 
of understanding the former language. The cross-language alignment of the entity identifiers was
done by two annotators. 

Table~\ref{tab:datasets} provides more detailed quantitative data about the test annotated datasets. A breakdown 
of the entity mention figures with respect to entity type is given in Table~\ref{tab:datasets-2}. 

It is worthwhile mentioning that for the sake of compliance with the named-entity recognition guidelines described
in~\ref{sec:task} documents that included hard-to-decide entity mention annotations were excluded from the test 
datasets, e.g., ``\textit{ZagrebaÄke, SisaÄko-MoslavaÄke i KarlovaÄke Å¾upanije}'', being a conflated versions of 3 
named entities, namely, ``\textit{ZagrebaÄke Å¾upanije}'', ``\textit{SisaÄko-MoslavaÄke Å¾upanije}'' and ``\textit{KarlovaÄke Å¾upanije}''.

\begin{table}
\begin{center}
\begin{footnotesize}
%\begin{tabular}{|l|c|c|c|c|c|c|}
\begin{tabular}{lcccc}
\toprule 
 & \multicolumn{2}{c}{\textbf{{\sc Trump}}} & \multicolumn{2}{c}{\textbf{{\sc ECommission}}} \\
\cmidrule(lr){2-3}
\cmidrule(lr){4-5}
Language &  \#\,docs & \#\,ment & \#\,docs & \#\,ment \\
\midrule
Croatian & 25 & 525 & 25 & 436 \\
Czech & 25 & 479  & 25 & 417 \\
Polish & 25 & 692  & 24 & 466 \\
Russian & 26 & 331  & 24 & 385 \\
Slovak  & 24 & 453  & 25 & 374 \\
Slovene & 24 & 474  & 26 & 434 \\
Ukrainian & 28 & 337  & 54 & 1078 \\
\midrule
Total & 177 & 3291  & 203 & 3588 \\


\bottomrule
\end{tabular}
\end{footnotesize}
\end{center}
\caption{Quantitative data about the test datasets. \#docs and \#ment refer to the number of documents and named-entity mention annotations.}
\label{tab:datasets}
\end{table}

\begin{table}
\begin{center}
\begin{footnotesize}
%\begin{tabular}{|l|c|c|c|c|c|c|}
\begin{tabular}{lcc}
\toprule 
 Entity type & {\textbf{{\sc Trump}}} & {\textbf{{\sc ECommission}}} \\
\midrule
ORG & 18.3\% & 48.4\% \\
LOC & 26.9\% & 29.1\% \\
PER & 48.4\% & 11.9\% \\
MISC & 6.4\% & 9.6\% \\
\bottomrule
\end{tabular}
\end{footnotesize}
\end{center}
\caption{Breakdown of the annotations according to the entity type.}
\label{tab:datasets-2}
\end{table}

\section{Evaluation Methodology}
\label{sec:evaluation}

Named entity recognition (exact case-insensitive matching) and  Name Normalization (sometimes called âlemmatizationâ) tasks were evaluated in terms of precision, recall, and F1 scores. In particular, as regards named entity recognition, two types of evaluations were carried out:

\begin{itemize}

\item \textbf{Relaxed evaluation}: an entity mentioned in a given document is considered to be extracted correctly if the system response includes at least one annotation of a named mention of this entity (regardless whether the extracted mention is base form);

\item \textbf{Strict evaluation}: the system response should include exactly one annotation for each unique form of a named mention of an entity that is referred to in a given document, i.e., capturing and listing all variants of an entity is required.

\end{itemize}

In the context of the evaluation we considered various level of granularity, i.e., the performance for: 
(a) all NE types and all languages, (b) each particular NE type and all languages, (c) all NE types for 
each language, and (d) each particular NE type per language. 

As regards the evaluation of document-level and cross-language entity matching task we have adapted the 
Link-Based Entity-Aware metric (LEA)~\cite{DBLP:conf-acl-Moosavi016} which considers how important the entity is and how well it is resolved. LEA is defined as follows. Let $K = \{k_1,k_2,\ldots,k_{|K|} \}$ and $R = \{r_1,r_2,\ldots,r_{|R|} \}$ denote the key entity set and the response entity set respectively, i.e., $k_i \in K$ ($r_i \in R$) stand for set of mentions
of the same entity in the key entity set (response entity set). LEA recall and precision are then defined as follows:

\begin{equation*}
Recall = \frac{\sum_{k_{i} \in K} (imp(k_i) \times res(k_{i}))}
              {\sum_{k_{z} \in K} imp(k_{z})}
\end{equation*}

\begin{equation*}
Precision = \frac{\sum_{r_{i} \in R} (imp(r_i) \times res(r_{i}))}
              {\sum_{r_{z} \in R} imp(r_{z})}
\end{equation*}

\noindent where $imp$ and $res$ denote measure of importance and resolution score for an entity respectively.
In our setting we define $imp(e) = log|e|+1$ for an entity $e$ (in $K$ or $R$), i.e., the more mentions 
an entity has the more important it is. However, in order not to bias the importance of the more frequent entities $log$ 
is used. The resolution score of key entity $k_i$ is computed as the fraction of correctly resolved co-reference 
links of $k_i$: 

\begin{equation*}
res(k_i) = \sum_{r_{j} \in R} \frac{link(k_{i} \cap r_{j})}{link(k_{i})}
\end{equation*}

\noindent where $link(e) = (|e| \times (|e|-1))/2$ stands for the number of unique 
co-reference links in $e$. For each $k_i$, LEA checks all response entities to check whether 
they are partial matches for $k_i$. Analogously, the resolution score of response entity $r_i$ is
computed as fraction of co-reference links in $r_i$ that are extracted correctly:

\begin{equation*}
res(r_i) = \sum_{k_{j} \in K} \frac{link(r_{i} \cap k_{j})}{link(r_{i})}
\end{equation*}

There are various benefits of using LEA, e.g., LEA considers resolved co-reference relations 
instead of resolved mentions and has more discriminative power than other metrics used
for co-reference resolution evaluation~\cite{DBLP:conf-acl-Moosavi016}.

It is important to stress at this stage that the evaluation was carried out in ``case-insensitive'' 
mode, all named mentions in system response and test corpora were lowercased.

\section{Participant Systems}
\label{sec:participants}
 
11 teams from 7 different countries (Czech Republic, Germany, India, Poland, Russia, Slovenia and USA) signed up for the evaluation task and received the trial datasets. However, presumably due to the complexity of the task and relatively short
time available for elaborating a solution, only 3 (\textbf{AS OF NOW WE RECEIVED 3 short system descriptions - 16.02}) teams submitted results. A total of \textbf{XX} unique runs were submitted: \todo{ammend once we know}

The {\sc DeepNLP} team~\footnote{https://github.com/tindzk/bsnlp} learns entities and their base forms from 
Wikipedia data dumps.~\footnote{https://dumps.wikimedia.org/} A character-level recurrent neural network~\cite{DBLP:conf/aaai/KimJSR16} 
is trained to predict which characters belong to an entity. A second neural network is trained on inflected entities and predicts their base forms.

\todo{DeepNLP team needs to be contacted to confirm the description since they mentioned they might change the strategy slightly.}

JHU/APL team only attempted the NER and Entity Matching sub-tasks. They employed a statistical tagger called 
SVMLattice~\cite{Mayfield:2003:LTU:956863.956921}, with NER labels inferred by projecting English tags across bitext. 
The Illinois tagger~\cite{Ratinov:2009:DCM:1596374.1596399} was used for English. A rule-based entity clusterer 
called "kripke" was used for Entity Matching~\cite{DBLP:conf/tac/McNameeMFL13}.

{\sc G4.19 Research Group} adapted Liner2~\cite{series/sci/MarcinczukKJ13} - a generic framework 
which can be used to solve various tasks based on sequence labeling, which is equipped
with a set of modules (based on statistical models, dictionaries, rules and heuristics) 
which recognize and annotate certain types of phrases. The details of tuning Liner2 to
tackle the shared task is described in~\cite{marcinczuk:2017}.

\section{Evaluation Results}
\label{sec:results}

\todo{content and presentation yet to be elaborated}

\section{Conclusions}
\label{sec:conclusions}

\todo{TODO:}

\begin{itemize}

\item write something about the availability of the data for further use (remember copyright issues). e.g.,
available via SIGSLAV 

\item future task extensions, more corpora, more languages, improving the guidelines (we have some inaccuracies
in the gudielines on how to deal with certain phenomena, etc., bla bla

\item emphasize that this shared task was to kick-off research in this area for Slavic languages

\end{itemize}


\section*{Acknowledgments}


We would like to thank Katja Zupan from the Department of Knowledge Technologies from
JoÅ¾ef Stefan Institute in Slovenia, Anastasia Stepanova from the State University of New York at Buffalo, 
and Olga Kanishcheva, Kateryna Klymenkova, Ekaterina Yurieva from the National Technical University 
(Kharkiv Polytechnic Institute), who contributed to preparation of the Slovenian, Russian and Ukrainian test 
datasets. We are also indebted to TomaÅ¾ Erjavec from the Department of Knowledge Technologies from JoÅ¾ef Stefan 
Institute in Slovenia, who contributed with various ideas.




\bibliographystyle{eacl2017}
\bibliography{sharedBSNLP2017}




\end{document}
