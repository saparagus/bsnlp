%
% File eacl2017.tex
%
%% Based on the style files for ACL-2016
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{eacl2017}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{amsmath}
%\usepackage{polski}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{booktabs}
\usepackage{color}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{fixltx2e}

%\usepackage{todonotes}%remove this when all todos are gone
\usepackage{todonotes}

\newcommand{\alert}[1]{\textcolor{red}{#1}}

\newcommand{\comment}[1]{}
\newcommand{\COMMENT}[1]{\comment{#1}}

\newcommand{\nocomment}[1]{{#1}}
\newcommand{\SHORTEN}[1]{}
\newcommand{\textexample}[1]{{\em #1}}
\newcommand{\dontsubmit}[1]{{\color{blue} #1}} %Change this to {} once ready for submission
%\newcommand{\dontsubmit}[1]{} %Change this to {} once ready for submission
\newcommand{\URGENT}[1]{\dontsubmit{\bf \color{red} #1}} %{{\bf \color{red} #1}}% % replace by {} when done




\eaclfinalcopy % Uncomment this line for the final submission
%\def\eaclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}


% Shared Task on Multilingual Named Entity Recognition, Normalisation and
% cross-lingual Matching for Slavic Languages

\title{The First Cross-Lingual Challenge on Recognition,
  Normalization
  \\ and Matching of Named Entities in Slavic Languages}


  \author{Jakub Piskorski\textsuperscript{1},
  Lidia Pivovarova\textsuperscript{2},
  Jan Šnajder\textsuperscript{3},
  Josef Steinberger\textsuperscript{4},
  Roman Yangarber\textsuperscript{2} \\
  %
  \textsuperscript{1}Joint Research Centre,
  \comment{Via Enrico Fermi 2749, 21027}Ispra\comment{ (VA)}, Italy,
  {\small \tt first.last@jrc.ec.europa.eu} \\
  %
  \textsuperscript{2}University of Helsinki, Finland, {\small \tt
  first.last@cs.helsinki.fi}\\
  %
    \textsuperscript{3}University of Zagreb, Croatia, {\small \tt
    first.last@fer.hr} \\
  %
  \textsuperscript{4}University of West Bohemia, Czech Republic, 
  {\small \tt jstein@kiv.zcu.cz} 
  %
  }

\comment{author 
  Jakub Piskorski \\
  Joint Research Centre \\
  Via Enrico Fermi 2749 \\
  21027 Ispra (VA), Italy \\
  {\small \tt first.last@jrc.ec.europa.eu} \\\And
  Lidia Pivovarova \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And 
	Jan Šnajder \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And	
	Josef Steinberger \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
	Roman Yangarber \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} 
	\\}

\date{}

\begin{document}
\maketitle
\begin{abstract}

  This paper describes the outcomes of the First Multilingual Named Entity Challenge in Slavic
  Languages.  The Challenge targets recognizing mentions of named entities in web documents,
  their normalization/lemmatization, and cross-lingual matching.  The Challenge was organized
  in the context of the 6th Balto-Slavic Natural Language Processing Workshop, co-located with
  the EACL-2017 conference.  Eleven teams registered for the evaluation, two of which
  submitted results on schedule, due to the complexity of the tasks and short time available
  for elaborating a solution.  The reported evaluation figures reflect the relatively higher
  level of complexity of named entity tasks in the context of Slavic languages.  Since the
  Challenge extends beyond the date of the publication of this paper, updates to the results
  of the participating systems can be found on the official web page of the Challenge.

\end{abstract}

\section{Introduction}
\label{sec:intro}

Due to the rich inflection, derivation, free word order, and other morphological and syntactic
phenomena exhibited by Slavic languages, analysis of named entities (NEs) in these languages
poses a challenging
task~\cite{Przepiorkowski:2007:SIE:1567545.1567547,journals/ir/PiskorskiWS09}.  Fostering
research and development on detection and lemmatization of NEs---and the closely related
problem of entity linking---is of paramount importance for enabling effective multilingual and
cross-lingual information access in these languages.

This paper describes the outcomes of the first shared task on multilingual named entity
recognition {(NER)} that aims at recognizing mentions of named entities in web documents in
Slavic languages, their normalization/lemmatization, and cross-lingual matching.  The task
initially covers seven languages and four types of NEs: person, location, organization, and
miscellaneous, where the last category covers all other types of named entities, e.g., event
or product.  The input text collection consists of documents in {seven Slavic languages}
collected from the web, each collection revolving around a certain ``focus'' entity.  The main
rationale of such a setup is to foster development of ``all-rounder'' NER and cross-lingual
entity matching solutions that are not tailored to specific, narrow domains.  The shared task
was organized in the context of the 6th Balto-Slavic Natural Language Processing Workshop
co-located with the EACL 2017 conference.

Similar shared tasks have been organized previously.  The first {\em non-English} monolingual
NER evaluations---covering Chinese, Japanese, Spanish, and Arabic---were carried out in the
context of the Message Understanding Conferences (MUCs)~\cite{chinchor:98} and the ACE
Programme~\cite{conf/lrec/DoddingtonMPRSW04}.  The first shared task focusing on
\emph{multilingual} named entity recognition, which covered some European languages, including
Spanish, German, and Dutch, was organized in the context of CoNLL
conferences~\cite{TjongKimSang:2002:ICS:1118853.1118877,TjongKimSang:2003:ICS:1119176.1119195}.
The NE types covered in these campaigns were similar to the NE types covered in our Challenge.
%
Also related to our task is the Entity Discovery and Linking (EDL)
track~\cite{ji:ea:2014,ji:ea:2015} of the NIST Text Analysis Conferences (TAC).  EDL aimed to
extract entity mentions from a collection of textual documents in multiple languages (English,
Chinese, and Spanish), and to partition the entities into cross-document equivalence classes,
by either linking mentions to a knowledge base or directly clustering them.  An important
difference between EDL and our task is that we do not link entities to a knowledge base.

Related to cross-lingual NE recognition is NE transliteration, i.e., linking NEs across
languages that use different scripts.  A series of NE Transliteration Shared Tasks were
organized as a part of NEWS---Named Entity Workshops---\cite{duan2016report}, focusing mostly
on Indian and Asian languages.  In 2010, the NEWS Workshop included a shared task on
Transliteration Mining~\cite{kumaran2010report}, i.e., mining of names from parallel corpora.
This task included corpora in English, Chinese, Tamil, Russian, and Arabic.

Prior work targeting NEs specifically for Slavic languages includes tools for NE recognition
for Croatian \cite{karan2013croner,ljubesic2013combining}, a tool tailored for NE recognition
in Croatian tweets \cite{baksa2017tagging}, a manually annotated NE corpus for Croatian
\cite{agic2014setimes}, tools for NE recognition in
Slovene~\cite{stajner2013razpoznavanje,ljubesic2013combining}, a Czech corpus of 11,000
manually annotated NEs~\cite{vsevvcikova2007named}, NER tools for Czech~\cite{Konkol:2013},
tools and resources for fine-grained annotation of NEs in the National Corpus of
Polish~\cite{was:etal:10,sav:pis:11} and a recent shared task on NE Recognition in
Russian~\cite{alexeeva2016factrueval}.

To the best of our knowledge, the shared task described in this paper is the first attempt at
multilingual name recognition, normalization, and cross-lingual entity matching that covers a
large number of Slavic languages.

This paper is organized as follows.  Section~\ref{sec:task} describes the task;
Section~\ref{sec:annotation} describes the annotation of the dataset.  The evaluation
methodology is introduced in Section~\ref{sec:evaluation}.  Participant systems are described
in Section~\ref{sec:participants} and the results obtained by these systems are presented in
Section~\ref{sec:results}.  Finally, lessons learnt and conclusions are discussed in
Section~\ref{sec:conclusions}.

\section{Task Description}
\label{sec:task}


The data for the shared task consists of text documents {in seven Slavic languages: Croatian,
  Czech, Polish, Russian, Slovak, Slovene, and Ukrainian. The documents focus around a certain
  entity---e.g., a person or an organization. The documents were obtained from the web, by
  posing a query to a search engine and parsing the HTML of the retrieved documents.}

The task {is} to recognize, classify, and ``normalize'' all named-entity mentions in each of
the documents, and to link across languages all named mentions referring to the same
real-world entity.

Formally, the Multilingual Named Entity Recognition task includes three
sub-tasks:

\begin{itemize}

\item \textbf{Named Entity Mention Detection and Classification.}
  Recognizing all unique named mentions of entities of four types:
  persons (\textsc{Per}), organizations (\textsc{Org}), locations (\textsc{Loc}), miscellaneous
  (\textsc{Misc}), the last covering mentions of all other types of named
  entities, e.g., products, events, etc.

\item \textbf{Name Normalization.} {Mapping each named mention of an entity to its
    corresponding {\em base form}.  By ``base form'' we generally mean the lemma (``dictionary
    form'') of the inflected word-form.  In some cases normalization should go beyond
    inflection and transform a derived word into a base word's lemma, e.g., in case of
    personal possessives (see below).  Multi-word names should be normalized to the {\em
      canonical} multi-word expression, rather than a sequence of lemmas of the words making
    up the multi-word expression.}

\item \textbf{Entity Matching.} Assigning an identifier (ID) to each detected named mention of
  an entity, in such a way that mentions of entities referring to the same real-world entity
  should be assigned the same ID (referred to as the cross-lingual ID).

\end{itemize}

\noindent The task does not require positional information of the name entity mentions.
Consequently, for all occurrences of the same form of a NE mention (e.g., inflected variant,
acronym, or abbreviation) within the same document no more than one annotation should be
returned.\footnote{Unless the different occurrences have different entity types (different
  readings) assigned to them, which is rare.}  Furthermore, distinguishing case information is
not necessary since the evaluation is case-insensitive.  In particular, if the text includes
lowercase, uppercase or mixed-case variants of the same entity, the system should produce only
one annotation for all of these mentions.  For instance, for ``\textit{ISIS}'',
``\textit{isis}'', and ``\textit{Isis}'' (provided that they refer to the same NE type), only
one annotation should be returned.  Note that the recognition of nominal or pronominal
mentions of entities is not part of the task.

\subsection{Named Entity Classes}

The task defines the following four NE classes.

\begin{description}

\item[Person names (\textsc{Per}).] 
  
  {Names of real persons (and fictional characters).} Person
  names should not include titles, honorifics, and functions/positions. For example, in the
  text fragment ``\textit{\dots CEO Dr.~Jan Kowalski\dots}'', only ``\textit{Jan Kowalski"}'' is
  recognized as a person name.  Initials and pseudonyms are considered named mentions of
  persons and should be recognized.  Similarly, named references to groups of people (that do
  not have a formal organization unifying them) should also be recognized, e.g.,
  ``\textit{Ukrainians}.'' In this context, mentions of a single member belonging to such
  groups, e.g., ``\textit{Ukrainian},'' should be assigned the same cross-lingual ID as plural
  mentions, i.e., ``\textit{Ukrainians}'' and ``\textit{Ukrainian}'' when referring to the
  nation should be assigned the same cross-lingual ID.

  Personal possessives derived from a person name should be classified as a person, and the
  base form of the corresponding person name should be extracted. For instance, for
  ``\textit{Trumpov tweet}'' (Croatian) it is expected to recognize ``\textit{Trumpov}'' and
  classify it as PER, with the base form ``\textit{Trump}.''


\item[Locations (\textsc{Loc}).]

  All toponyms and geopolitical entities (cities, counties, provinces, countries, regions,
  bodies of water, land formations, etc.), including named mentions of {\em facilities} (e.g.,
  stadiums, parks, museums, theaters, hotels, hospitals, transportation hubs, churches,
  railroads, bridges, and similar facilities).

  In case named mentions of facilities may also refer to an organization, the \textsc{Loc} tag
  should be used.  For example, from the text phrase ``\textit{The Schipol Airport has
    acquired new electronic gates}'' the mention ``\textit{The Schipol Airport}'' should be
  extracted and classified as \textsc{Loc}.

\item[Organizations (\textsc{Org}).]

  All kinds of organizations: political parties, public institutions, international
  organizations, companies, religious organizations, sport organizations, educational and
  research institutions, etc.

  Organization designators and potential mentions of the seat of the organization are
  considered to be part of the organization name. For instance, from the text fragment
  ``\textit{Citi Handlowy w Poznaniu}'' (a bank in Poznań), the full phrase ``\textit{Citi
    Handlowy w Poznaniu}'' should be extracted.
			
  When a company name is used to refer to a service (e.g., ``\textit{na Twiterze}'' (Polish
  for ``on Twitter''), the mention of ``\textit{Twitter}'' is considered to refer to a
  service/product and should be tagged as \textsc{Misc}.  However, when a company name is
  referring to a service which expresses the opinion of the company, e.g., ``\textit{Fox
    News}'', it should be tagged as \textsc{Org}.

\item[Miscellaneous (\textsc{Misc}).]

  All other named mentions of entities, e.g., product names---e.g., ``\textit{Motorola Moto
    X}''), events (conferences, concerts, natural disasters, holidays, e.g., ``\textit{Święta
    Bożego Narodzenia}'' (Polish for ``Christmas''), etc.

  This category does not include temporal and numerical expressions, as well as identifiers
  such as email addresses, URLs, postal addresses, etc.

\end{description}



\subsection{Complex and Ambiguous Entities}

In case of complex named entities, consisting of nested named entities, only the {\em
  top-most} entity should be recognized. For example, from the text string ``\textit{George
  Washington University}'' one should not extract ``\textit{George Washington}'', but the
entire string.

In case one word-form (e.g., ``\textit{Washington}'') is used to refer to two different
real-world entities in different contexts in the same document (e.g., a person and a
location), the system should return two annotations, associated with different cross-lingual
IDs.


\subsection{System Input and Response}
\label{sec:protocol}

\paragraph{Input Document Format.}
\label{subsec:input}
%
Documents in the collection are represented in the following format.  The {first five
  lines} contain meta-data; the core text to be processed begins from the 6th line and runs
till the end of file.

\begin{small}
\begin{verbatim}
<DOCUMENT-ID>
<LANGUAGE>
<CREATION-DATE>
<URL>
<TITLE>
<TEXT>
\end{verbatim}
\end{small}

\noindent The \verb+<URL>+ field stores the origin from which the text document was retrieved.
The values of the meta-data fields were computed automatically
(see~Section~\ref{sec:annotation} for details). In particular, the values of
\verb+<CREATION-DATE>+ and \verb+<TITLE>+ were not provided for all documents, either due to
unavailability of such data or due to errors in web page parsing during the creation process.

\paragraph{System Response.}
\label{subsec:input}
%
For each input document, the systems should return one file as follows.  The first line should
contain only the \verb+<DOCUMENT-ID>+ field that corresponds to the input file.  Each
subsequent line should contain the following, tab-separated fields:

\begin{small}
\begin{verbatim}
<MENTION> TAB <BASE> TAB <CAT> TAB <ID>
\end{verbatim}
\end{small}

\noindent The value of the \verb+<MENTION>+ field should be the NE mention as it appears in
text.  The value of the \verb+<BASE>+ field should be the base form of the entity.  The
\verb+<CAT>+ and \verb+<ID>+ fields store information on the category of the entity
(\textsc{Org}, \textsc{Per}, \textsc{Loc}, or \textsc{Misc}) and cross-lingual identifier,
respectively.  The cross-lingual identifiers may consist of an arbitrary sequence of
alphanumeric characters.  An example of a system response (for a document in Polish) is given
below.

\resizebox{.9\linewidth}{!}{%
  \begin{tabular*}{.5\textwidth}{l }
    16 \\
    Podlascy Czeczeni\ \ \ \ \ \ Podlascy Czeczeni\ \ \ \ \ \ PER\ \ \ \ \ \ 1 \\
    ISIS\ \ \ \ \ \ ISIS\ \ \ \ \ \ ORG\ \ \ \ \ \ 2 \\
    Rosji\ \ \ \ \ \ Rosja\ \ \ \ \ \ LOC\ \ \ \ \ \ 3 \\
    Rosja\ \ \ \ \ \ Rosja\ \ \ \ \ \ LOC\ \ \ \ \ \ 3 \\
    Polsce\ \ \ \ \ \ Polska\ \ \ \ \ \ LOC\ \ \ \ \ \ 4 \\
    Warszawie\ \ \ \ \ \ Warszawa\ \ \ \ \ \ LOC\ \ \ \ \ \ 5 \\
    Magazynu Kuriera Porannego\ \ \ \ \ \ Magazyn Kuriera\textbackslash \\
    \ \ \ \ \ \ \ \ \ \ \ \  Porannego\ \ \ \ \ \ ORG\ \ \ \ \ \ 6 \\
  \end{tabular*}
}

\comment{
%\begin{small}
%\begin{verbatim}
16
Podlascy Czeczeni    Podlascy Czeczeni    PER    1
ISIS    ISIS    ORG    2
Rosji    Rosja    LOC    3
Polsce    Polska    LOC    4
Warszawie    Warszawa    LOC    5
Rosja    Rosja    LOC    3
Magazynu Kuriera Porannego \
      Magazyn Kuriera Porannego    ORG    6
%\end{verbatim}	    
}

\section{Data}
\label{sec:annotation}

\subsection{Trial Datasets}

The registered participants were provided two trial datasets: (1) a dataset related to {\em
  Beata Szydło}, the current prime minister of Poland, and (2) a dataset related to {\em
  ISIS}, the so-called ``Islamic State of Iraq and Syria'' terrorist group.  These datasets
consisted of 187 and 186 documents, respectively, with equal distribution of documents across
the seven languages of interest.

\subsection{Test Datasets}

Two datasets were prepared for evaluation, each consisting of documents extracted from the web
and related to a given entity.  One dataset contains documents related to {\em Donald Trump}, the
recently elected President of United States (henceforth referred to as {\sc Trump}), and the
second dataset contains documents related to the {\em European Commission} (henceforth referred to
as {\sc ECommission}).

The test datasets were created as follows.  For each ``focus'' entity, we posed a separate
search query to Google, in each of the seven target languages.  The query returned links to
documents only in the language of interest.  We extracted the first 100 links\footnote{Or
  fewer, in case the search engine did not return 100 links.} returned by the search engine,
removed duplicate links, downloaded the corresponding HTML pages---mainly news articles or
fragments thereof---and converted them into plain text, using a hybrid HTML parser.  This
process was done semi-automatically using the tool described in~\cite{Crawley:ea:2010}.  In
particular, some of the meta-data fields---i.e., creation date, title, URL---were automatically
computed using this tool.

HTML parsing resulted in texts that included not only the core text of a web page, but also
some additional pieces of text, e.g., a list of labels from a menu, user comments, etc., which
may not constitute well-formed utterances in the target language.  This occurred in a small
fraction of texts processed.  Some of these texts were included in the test dataset in order
to maintain the flavour of ``real-data.''  However, obvious HTML parser failure (e.g.,
extraction of JavaScript code, extraction of empty texts, etc.) were removed from the data
sets.  Some of the downloaded documents were additionally polished by removing erroneously
extracted boilerplate content.  The resulting set of partially ``cleaned'' documents were used
to select {\em circa} 20--25 documents for each language and topic, for the preparation of the
final test datasets.  Annotations for Croatian, Czech, Polish, Russian, and Slovene were made
by native speakers; annotations for Slovak were made by native speakers of Czech, capable of
understanding Slovak.  Annotations for Ukrainian were made partly by native speakers and
partly by near-native speakers of Ukrainian.  Cross-lingual alignment of the entity
identifiers was performed by two annotators.



Table~\ref{tab:datasets} provides more quantitative details about the annotated datasets.
Table~\ref{tab:datasets-2} gives the breakdown of entity classes.  It is noteworthy that a
high proportion of the annotated mentions have a base form that differs from the form
appearing in text.  For instance, for the {\sc Trump} dataset this figure is between 37.5\%
(Slovak) and 57.5\% (Croatian). Table~\ref{tab:inflected} provides examples of genitive forms 
of the name ``\textit{European Commission}'' that occurred in the {\sc ECommission} corpus frequently.

While normalization of the inflected forms in Table~\ref{tab:inflected} could be achieved
by lemmatization of each of the constituents of the noun phrase separately and then concatenating
the corresponding base forms together, many entity mentions in the test dataset are complex noun phrases, 
whose lemmatization requires detection of inner syntactic structure. For instance, the inflected form 
of the Polish proper name \textit{Europejskiego Funduszu Rozwoju Regionalnego} 
(\emph{European\textsubscript{GEN} Fund\textsubscript{GEN} Development\textsubscript{GEN} Regional\textsubscript{GEN}}) consists of two basic genitive 
noun phrases, of which only the first one (``European Fund'') needs to be normalized, whereas the 
second (``Regional Development'') should remain unchanged. The corresponding base form
is ``\textit{Europejski Fundusz Rozwoju Regionalnego}''. Since in some Slavic languages
adjectives may precede or follow a noun in a noun phrase (like in the example above), 
detection of inner syntactic structure of complex proper names is not trivial, and thus complicates
the process of automated lemmatization.

It is worth mentioning that, for the sake of compliance with the NER guidelines in~Section
~\ref{sec:task}, documents that included hard-to-decide entity mention annotations were
excluded from the test datasets for the present.  {A case in point is a document in Croatian
  that contained the phrase ``\textit{Zagrebačka, Sisačko-Moslavačka i Karlovačka
    županija}''---a contracted version of three named entities (``\textit{Zagrebačka
    županija}'', ``\textit{Sisačko-Moslavačka županija}'', and ``\textit{Karlovačka
    županija}'') expressed using a head noun with three coordinate modifiers.}

\begin{table}
  \begin{center}
    \begin{footnotesize}
      % \begin{tabular}{|l|c|c|c|c|c|c|}
      \begin{tabular}{lcccc}
        \toprule 
        & \multicolumn{2}{c}{\textbf{{\sc Trump}}} & \multicolumn{2}{c}{\textbf{{\sc ECommission}}} \\
        \cmidrule(lr){2-3}
        \cmidrule(lr){4-5}
        Language &  \#\,docs & \#\,ment & \#\,docs & \#\,ment \\
        \midrule
        Croatian & 25 & 525 & 25 & 436 \\
        Czech & 25 & 479  & 25 & 417 \\
        Polish & 25 & 692  & 24 & 466 \\
        Russian & 26 & 331  & 24 & 385 \\
        Slovak  & 24 & 453  & 25 & 374 \\
        Slovene & 24 & 474  & 26 & 434 \\
        Ukrainian & 28 & 337  & 54 & 1078 \\
        \midrule
        Total & 177 & 3291  & 203 & 3588 \\

        \bottomrule
      \end{tabular}
    \end{footnotesize}
  \end{center}
  \caption{Quantitative data about the test datasets. {\em \#docs} and {\em \#ment} refer to
    the number of documents and NE mention annotations, respectively.} 
  \label{tab:datasets}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}
  \begin{center}
    \begin{footnotesize}
      % \begin{tabular}{|l|c|c|c|c|c|c|}
      \begin{tabular}{lcc}
        \toprule 
        Entity type & {\textbf{{\sc Trump}}} & {\textbf{{\sc ECommission}}} \\
        \midrule
	\textsc{Per} & 48.4\% & 11.9\% \\
	\textsc{Loc} & 26.9\% & 29.1\% \\
	\textsc{Org} & 18.3\% & 48.4\% \\
	\textsc{Misc} & \phantom{0}6.4\% & \phantom{0}9.6\% \\
        \bottomrule
      \end{tabular}
    \end{footnotesize}
  \end{center}
  \caption{Breakdown of the annotations according to the entity type.}
  \label{tab:datasets-2}
\end{table}

\begin{table}
  \begin{center}
    \begin{footnotesize}
      % \begin{tabular}{|l|c|c|c|c|c|c|}
      \begin{tabular}{lcc}
        \toprule 
        {Language} & {Genitive} & {Nominative}\\
        \midrule
				Croatian & Europske komisije & Europska komisija \\
				Czech & Komisji Europejskiej & Komisja Europejska \\
        Polish & Evropskou komisí & Evropská komise \\
        %Russian & Европейской комиссией & Европейская комиссия \\
        Slovene & Evropske komisije & Evropska komisija \\
        Slovak & Európskej komisie & Európska komisia \\
        %Ukrainian & Європейської Комісії & Європейська Комісія \\
        \bottomrule
      \end{tabular}
    \end{footnotesize}
  \end{center}
  \caption{Inflected (genitive) forms of the name ``\textit{European Commission}'' that occurred in the test dataset.} 
  \label{tab:inflected}
\end{table}



\begin{table*}[t]
  \centering
  \setlength\tabcolsep{5pt}
  %\caption{Global caption}
  \begin{minipage}{\linewidth}
    \centering
    %\tablewidth=\textwidth
\scalebox{0.87}{
	\begin{tabular}{@{} l  l  l r l r l r l r l r  l r l r@{}}

\toprule
{\sc Trump}      &      & \multicolumn{14}{c}{\em Language}                                                                    \\
\cmidrule(lr){3-16}
{\it Phase}               & {\it Metric}          & cz       &      & hr   &      & pl   &      & ru   &      & sk   &      & sl   &      & ua   &        \\
\midrule
\multirow{6}{*}
{Recognition}       & Relaxed Partial & jhu      & 46.2 & jhu  & 52.4 & pw   & 66.6 & jhu  & 46.3 & jhu  & 46.8 & jhu  & 47.3 & jhu  & 38.8   \\
                    &                 &          &      &      &      & jhu  & 44.8 &      &      &      &      &      &      &      &        \\
\cmidrule{2-16}
                    & Relaxed Exact   & jhu      & 46.1 & jhu  & 50.8 & pw   & 66.1 & jhu  & 43.1 & jhu  & 46.2 & jhu  & 46.0 & jhu  & 37.3   \\
                    &                 &          &      &      &      & jhu  & 43.4 &      &      &      &      &      &      &      &        \\
\cmidrule{2-16}
                    & Strict          & jhu      & 46.1 & jhu  & 50.4 & pw   & 66.6 & jhu  & 41.8 & jhu  & 47.0 & jhu  & 46.2 & jhu  & 33.2   \\
                    &                 &          &      &      &      & jhu  & 41.0 &      &      &      &      &      &      &      &        \\
\midrule
Normalization       &                 &          &      &      &      & pw   & 60.5 &      &      &      &      &      &      &      &        \\
\midrule
\multirow{3}{*}
{Entity matching}       & Document level  & jhu      &  5.4 & jhu  &  7.3 & jhu  &  6.3 & jhu  & 11.2 & jhu  & 10.1 & jhu  &  9.5 & jhu  &  0.0   \\
                        &                 &          &      &      &      & pw  &  10.8 &      &      &      &      &      &    &    &     \\
\cmidrule{2-16}
                    & Single language & jhu      & 19.3 & jhu  & 17.6 & jhu  & 18.2 & jhu  & 18.9 & jhu  & 22.6 & jhu  & 28.7 & jhu  & 10.7   \\
                    &                 &          &      &      &      & pw   &  4.9 &      &      &      &      &      &      &     &     \\

										
\cmidrule{2-16}
                    & Cross-lingual   & jhu      &  9.0 &  \multicolumn{12}{c}{}                                                             \\
\toprule
{\sc ECommission}      &        & \multicolumn{14}{c}{\em Language}                                                                    \\
\cmidrule(lr){3-16}
{\it Phase}               & {\it Metric}          & cz       &      & hr   &      & pl   &      & ru   &      & sk   &      & sl   &      & ua   &        \\
\midrule
\multirow{6}{*}
{Recognition}       & Relaxed Partial & jhu      & 47.6 & jhu  & 45.9 & pw   & 61.8 & jhu  & 46.0 & jhu  & 49.1 & jhu  & 47.9 & jhu  & 18.4   \\
                    &                 &          &      &      &      & jhu  & 47.3 &      &      &      &      &      &      &      &        \\
\cmidrule{2-16}
                    & Relaxed Exact   & jhu      & 44.4 & jhu  & 43.1 & pw   & 60.9 & jhu  & 44.1 & jhu  & 46.4 & jhu  & 43.9 & jhu  & 14.7   \\
                    &                 &          &      &      &      & jhu  & 42.4 &      &      &      &      &      &      &      &        \\
\cmidrule{2-16}
                    & Strict          & jhu      & 47.2 & jhu  & 46.2 & pw   & 61.1 & jhu  & 46.5 & jhu  & 46.1 & jhu  & 47.8 & jhu  & 10.8   \\
                    &                 &          &      &      &      & jhu  & 44.8 &      &      &      &      &      &      &      &        \\
\midrule
Normalization       &                 &          &      &      &      & pw   & 48.3 &      &      &      &      &      &      &      &        \\
\midrule
\multirow{3}{*}
{Entity Matching}       & Document level  & jhu      & 25.0 & jhu  & 16.0 & jhu  & 13.7 & jhu  & 13.7 & jhu  & 13.1 & jhu  & 36.8 & jhu  &  0.6   \\
                        &                 &          &      &      &      & pw   &  13.4    &      &      &      &      &      &   &   &     \\
\cmidrule{2-16}
                    & Single language & jhu      & 27.3 & jhu  & 22.1 & jhu  & 17.5 & jhu  & 24.9 & jhu  & 30.6 & jhu  & 32.2 & jhu  &  4.8   \\
                        &                 &          &      &      &      & pw   &  4.6    &      &      &      &      &      &   &   &     \\										
\cmidrule{2-16}
                    & Cross-lingual   & jhu      &  2.6 &  \multicolumn{12}{c}{}                                                             \\
\bottomrule
      \end{tabular}
}
    \caption{Evaluation results across all scenarios and languages.}
    \label{tab:eval-results}

  \end{minipage}%
%  \hfill
\end{table*}



\section{Evaluation Methodology}
\label{sec:evaluation}

The NER task (exact case-insensitive matching) and Name Normalization task (also called
``lemmatization'') were evaluated in terms of precision, recall, and F1-scores.  In
particular, for NER, two types of evaluations were carried out:

\begin{itemize}

\item \textbf{Relaxed evaluation}: An entity mentioned in a given document is considered to be
  extracted correctly if the system response includes at least one annotation of a named
  mention of this entity (regardless whether the extracted mention is in base form);

\item \textbf{Strict evaluation}: The system response should include exactly one annotation
  for {\em each} unique form of a named mention of an entity in a given document, i.e.,
  capturing and listing all variants of an entity is required.

\end{itemize}

\noindent In relaxed evaluation mode we additionally distinguish between \textit{exact} and
\textit{partial matching}, i.e., in the case of the latter an entity mentioned in a given
document is considered to be extracted correctly if the system response includes at least one
partial match of a named mention of this entity.

In the evaluation we consider various levels of granularity, i.e., the performance for: (a) all
NE types and all languages, (b) each particular NE type and all languages, (c) all NE types
for each language, and (d) each particular NE type per language.


In the name normalization sub-task, only correctly recognized entity mentions in the system
response and only those that were normalized (on both the annotation and system's sides) are
taken into account.  Formally, let $\emph{correct}_{N}$ denote the number of all correctly recognized
entity mentions for which the system returned a correct base form.  Let $\mathit{key}_{N}$ denote the
number of all normalized entity mentions in the gold-standard answer key and $\mathit{response}_{N}$
denote the number of all normalized entity mentions in the system's response. We define
precision and recall for the name normalization task as:
%
\begin{equation*}
	\mathit{Recall_{N}} = \frac{\mathit{corrrect}_{N}}{\mathit{key}_{N}}              
\end{equation*}

\begin{equation*}
	\mathit{Precision_{N}} = \frac{\mathit{corrrect}_{N}}{\mathit{response}_{N}}              
\end{equation*}


In evaluating the document-level, single-language and cross-lingual entity matching task we
have adapted the Link-Based Entity-Aware metric (LEA)~\cite{DBLP:conf-acl-Moosavi016} which
considers how important the entity is and how well it is resolved. 
%
LEA is defined as follows. 
%
Let $K = \{k_1,k_2,\ldots,k_{|K|} \}$ and $R = \{r_1,r_2,\ldots,r_{|R|} \}$ denote the key
entity set and the response entity set, respectively, i.e., $k_i \in K$ ($r_i \in R$) stand
for set of mentions of the same entity in the key entity set (response entity set). LEA recall
and precision are then defined as follows:
%
\begin{equation*}
	\mathit{\mathit{Recall}_{LEA}} = \frac{\sum_{k_{i} \in K} (\mathit{imp}(k_i) \times \mathit{res}(k_{i}))}
              {\sum_{k_{z} \in K} imp(k_{z})}
\end{equation*}

\begin{equation*}
	\mathit{\mathit{Precision}_{LEA}} = \frac{\sum_{r_{i} \in R} (\mathit{imp}(r_i) \times \mathit{res}(r_{i}))}
              {\sum_{r_{z} \in R} imp(r_{z})}
\end{equation*}

\noindent where $\mathit{imp}$ and $\mathit{res}$ denote the measure of importance and the
resolution score for an entity, respectively.  In our setting, we define
$\mathit{imp}(e) = \log_{2}|e|$ for an entity $e$ (in $K$ or $R$), $|e|$ is the number of
mentions of $e$---i.e., the more mentions an entity has the more important it is.  To avoid
biasing the importance of the more frequent entities $\log$ is used.  The resolution score of
key entity $k_i$ is computed as the fraction of correctly resolved co-reference links of
$k_i$:
%
\begin{equation*}
	\mathit{res}(k_i) = \sum_{r_{j} \in R} \frac{\mathit{link}(k_{i} \cap r_{j})}{\mathit{link}(k_{i})}
\end{equation*}

\noindent where $\mathit{link}(e) = (|e| \times (|e|-1))/2$ is the number
of unique co-reference links in $e$.  For each $k_i$, LEA checks all
response entities to check whether they are partial matches for
$k_i$. Analogously, the resolution score of response entity $r_i$ is
computed as the fraction of co-reference links in $r_i$ that are
extracted correctly:
%
\begin{equation*}
	\mathit{res}(r_i) = \sum_{k_{j} \in K} \frac{\mathit{link}(r_{i} \cap k_{j})}{\mathit{link}(r_{i})}
\end{equation*}

Using LEA brings several benefits.  For example, LEA considers resolved
co-reference relations instead of resolved mentions and has more
discriminative power than other metrics used for evaluation of
co-reference resolution~\cite{DBLP:conf-acl-Moosavi016}.

It is important to note at this stage that the evaluation was carried out
in ``case-insensitive'' mode: all named mentions in system response and
test corpora were lowercased.


\section{Participant Systems}
\label{sec:participants}
 
Eleven teams from seven countries---Czech Republic, Germany, India,
Poland, Russia, Slovenia, and USA---registered for the evaluation task and
received the trial datasets.  Due to the complexity of the task and
relatively short time available to create a working solution, only two
teams submitted results within the deadline.  A total of two unique runs
were submitted.

JHU/APL team attempted the NER and Entity Matching sub-tasks.  They
employed a statistical tagger called
SVMLattice~\cite{Mayfield:2003:LTU:956863.956921}, with NER labels
inferred by projecting English tags across bitext.  The Illinois
tagger~\cite{Ratinov:2009:DCM:1596374.1596399} was used for English. A
rule-based entity clusterer called ``kripke'' was used for Entity
Matching~\cite{DBLP:conf/tac/McNameeMFL13}.
%
The team (code ``{\em jhu}'') attempted all languages available in the
Challenge. More details can be found in~\cite{mayfield:2017}.

The {G4.19 Research Group} adapted
Liner2~\cite{series/sci/MarcinczukKJ13}---a generic framework which can
be used to solve various tasks based on sequence labeling, which is
equipped with a set of modules (based on statistical models,
dictionaries, rules and heuristics) which recognize and annotate certain
types of phrases.  The details of tuning Liner2 to tackle the shared task
are described in~\cite{marcinczuk:2017}.
%
The team (code ``{\em pw}'') attempted only the Polish-language Challenge.

\comment{
The {\sc DeepNLP} team\footnote{\tt https://github.com/tindzk/bsnlp}
learns entities and their base forms from Wikipedia data
dumps.\footnote{\tt https://dumps.wikimedia.org/} A character-level
recurrent neural network~\cite{DBLP:conf/aaai/KimJSR16} is trained to
predict which characters belong to an entity. A second neural network is
trained on inflected entities and predicts their base forms.
}

The above systems met the deadline to participate in the first run of the
Challenge---Phase I.
%
Since the Challenge aroused significant interest in the research
community, it was extended into Phase II, with a new deadline for
submitting system responses, beyond the time of publication of this
paper.  Please refer to the Challenge web
site\footnote{http://bsnlp.cs.helsinki.fi/shared\_task.html} for
information on the current status, systems tested, and their performance.

\section{Evaluation Results}
\label{sec:results}

The results of the runs submitted for Phase I are presented in
Table~\ref{tab:eval-results}.  The figures provided for the recognition
are micro-averaged F1-scores.

For normalization, we report F1-scores, using the $\mathit{Recall}_{N}$ and
$\mathit{Precision}_{N}$ definitions from Section~\ref{sec:evaluation}, computed
for entity mentions for which the annotation or system response contains
a different base form compared to the surface form.  This evaluation
includes only correctly recognized entity mentions to suppress the
influence of entity recognition performance.

Lastly, for entity matching, the micro-averaged F1-scores are provided,
computed using LEA precision and recall values (see
Section~\ref{sec:evaluation}).

System {\em pw} performed substantially better on Polish than system {\em
  jhu}.

Considering the entity types, performance was overall better for \textsc{Loc} and
\textsc{Per}, and substantially lower for \textsc{Org} and \textsc{Misc}, which is not unexpected. 
Table~\ref{tab:recognition-breakdown-trump} and~\ref{tab:recognition-breakdown-ec}
provide the overall average precision, recall, and F1 figures for the relaxed evaluation
with partial matching for {\sc Trump} and {\sc ECommission} scenario respectively.

Considering the tested languages and scenarios, system {\em jhu} achieved
best performance on {\sc Trump} in Croatian, its poorest performance was
on {\sc ECommission} in Ukrainian.  System {\em pw} performed better on
the {\sc Trump} scenario than on {\sc ECommission}.  Overall, the {\sc
  Trump} scenario appears to be easier, due to the mix of named entities
that predominate in the texts.  The {\sc ECommission} documents discuss
organizations with complex geo-political inter-relationships and
affiliations.

Furthermore, cross-lingual co-reference seems to be a difficult task.


\begin{table}
  \begin{center}
    \begin{footnotesize}
      % \begin{tabular}{|l|c|c|c|c|c|c|}
      \begin{tabular}{lccc}
        \toprule 
        Metric & Precision & Recall & F1 \\
        \midrule
	\textsc{Per} & 74.8 & 65.9 & 69.8 \\
	\textsc{Loc} & 73.0 & 75.4 & 74.2 \\
	\textsc{Org} & 47.1 & 22.1 & 30.0 \\
	\textsc{Misc} & 7.9 & 14.4 & 10.2 \\
        \bottomrule
      \end{tabular}
    \end{footnotesize}
  \end{center}
  \caption{Breakdown of the recognition performance according to the entity type for {\sc Trump} dataset.}
  \label{tab:recognition-breakdown-trump}
\end{table}

\begin{table}
  \begin{center}
    \begin{footnotesize}
      % \begin{tabular}{|l|c|c|c|c|c|c|}
      \begin{tabular}{lccc}
        \toprule 
        Metric & Precision & Recall & F1 \\
        \midrule
	\textsc{Per} & 68.2 & 59.4 & 62.9 \\
	\textsc{Loc} & 73.1 & 57.8 & 64.5 \\
	\textsc{Org} & 45.0 & 49.0 & 46.6 \\
	\textsc{Misc} & 18.7 & 12.0 & 14.2 \\
        \bottomrule
      \end{tabular}
    \end{footnotesize}
  \end{center}
  \caption{Breakdown of the recognition performance according to the entity type for {\sc ECommission} dataset.}
  \label{tab:recognition-breakdown-ec}
\end{table}

\section{Conclusions}
\label{sec:conclusions}


This paper reports on the First multilingual named entity Challenge that
aims at recognizing mentions of named entities in web documents in Slavic
languages, their normalization/lemmatization, and cross-lingual matching.
Although the Challenge aroused substantial interest in the field, only
two teams submitted results on time, most likely due to the complexity of
the tasks and the short time available to finalize a solution.  While
drawing substantial conclusions from the evaluation of two systems is not
yet possible, we can observe though that the overall performance of the
two systems on hidden test sets revolving around a specific entity is
significantly lower than in the case of processing less-morphologically
complex languages.

To support research on NER-related tasks for Slavic languages, including
cross-lingual entity matching, the Challenge was extended into Phase II,
going beyond the date of the publication of this paper.  For the current
list of systems that has been evaluated on the different tasks and their
performance figures please refer to the shared task web page.

The test datasets, the corresponding annotations and various scripts used
for the evaluation purposes are made available on the shared task web
page as well.

We plan to extend the Challenge through provision of additional test
datasets in the future, involving new entities, in order to further boost
research on developing ``all-rounder'' NER solutions for processing
real-world texts in Slavic languages and carrying out cross-lingual
entity matching.  Furthermore, we plan to extend the set of the languages
covered, depending on the availability of annotators.  Finally, some work
will focus on the refining the NE annotation guidelines in order to
properly deal with particular phenomena, e.g., coordinated NEs and
contracted versions of multiple NEs, which were excluded from the first
test datasets.


\section*{Acknowledgments}


We thank Katja Zupan (Department of Knowledge Technologies, Jožef Stefan
Institute, Slovenia), Anastasia Stepanova (State University of New York,
Buffalo), Domagoj Alagić (TakeLab, University of Zagreb), and Olga
Kanishcheva, Kateryna Klymenkova, Ekaterina Yurieva (the National
Technical University, Kharkiv Polytechnic Institute), who contributed to
the preparation of the Slovenian, Russian, Croatian, and Ukrainian test
data.  
%
We are also grateful to Tomaž Erjavec from the Department of Knowledge
Technologies, Jožef Stefan Institute in Slovenia, who contributed various
ideas.
%
Work on Czech and Slovak was supported by Project MediaGist, EU's FP7
People Programme (Marie Curie Action), no. 630786.

The effort of organizing the shared task was supported by the Europe Media Monitoring (EMM) 
Project carried out by the Text and Data Mining Unit of the Joint Research Centre of the 
European Commission.

\bibliographystyle{eacl2017}
\bibliography{sharedBSNLP2017}




\end{document}
